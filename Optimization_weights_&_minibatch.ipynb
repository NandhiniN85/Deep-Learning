{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Optimization weights & minibatch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnWaNZiRJDom"
      },
      "source": [
        "from sklearn.datasets import load_iris\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Activation, Dense\r\n",
        "from keras import optimizers\r\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO1VcmwDJKC_"
      },
      "source": [
        "iris = load_iris()"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Habv7jaJM_v",
        "outputId": "bcfd8a71-0164-42e6-f479-c9561757bb0d"
      },
      "source": [
        "iris.keys()"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5JiD4kIJRi7"
      },
      "source": [
        "data = pd.DataFrame(iris['data'], columns = iris['feature_names'] )\r\n",
        "target = pd.DataFrame(iris['target'],columns = ['target'])"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyASeuuWJTq2"
      },
      "source": [
        "#combine the input predictors and target so that it can be split into training and testing\r\n",
        "data_target = data.join(target)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "sYYdmyWTJVtE",
        "outputId": "01531904-0830-4a55-8696-47611334f639"
      },
      "source": [
        "data_target.head()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
              "0                5.1               3.5  ...               0.2       0\n",
              "1                4.9               3.0  ...               0.2       0\n",
              "2                4.7               3.2  ...               0.2       0\n",
              "3                4.6               3.1  ...               0.2       0\n",
              "4                5.0               3.6  ...               0.2       0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcK2hT91JXAs",
        "outputId": "3c18d3c1-0b79-41bb-f978-0e48027e965b"
      },
      "source": [
        "data_target['target'].value_counts()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    50\n",
              "1    50\n",
              "0    50\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36ErKb9LJYj_"
      },
      "source": [
        "#for this implementaion, let's make the problem a binary. So considering only '0' and '1' as the target\r\n",
        "X = np.array(data_target[(data_target['target'] == 0) | (data_target['target'] == 1)].drop('target', axis=1))\r\n",
        "y = np.array(data_target[(data_target['target'] == 0) | (data_target['target'] == 1)]['target']).reshape(100,1)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZgUoce-Jaeh",
        "outputId": "3f596de3-1922-4dd2-c23c-369fc3d3c9c8"
      },
      "source": [
        "train_x,test_x,train_y,test_y = train_test_split(X,y,test_size = 0.2)\r\n",
        "\r\n",
        "print('Shape of train_x:', train_x.shape)\r\n",
        "print('Shape of train_y:', train_y.shape)\r\n",
        "print('Shape of test_x:', test_x.shape)\r\n",
        "print('Shape of test_y:', test_y.shape)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train_x: (80, 4)\n",
            "Shape of train_y: (80, 1)\n",
            "Shape of test_x: (20, 4)\n",
            "Shape of test_y: (20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBVKIcDKJgwN"
      },
      "source": [
        "Splitting the data into 9 mini batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks9WaZ_6JeT7"
      },
      "source": [
        "batches = 9\r\n",
        "\r\n",
        "#get the total number of batches\r\n",
        "batch_count = train_x.shape[0] // batches\r\n",
        "\r\n",
        "batch_trainx = []\r\n",
        "batch_trainy = []\r\n",
        "\r\n",
        "for i in range(0, batch_count):\r\n",
        "  begin = i * batches\r\n",
        "  end = (i + 1) * batches\r\n",
        "\r\n",
        "  batch_trainx.append(train_x[begin:end])\r\n",
        "  batch_trainy.append(train_y[begin:end])\r\n",
        "\r\n",
        "#when the total count is not exactly divisible by batches\r\n",
        "left_out = train_x.shape[0] % batches\r\n",
        "\r\n",
        "if left_out != 0:\r\n",
        "  batch_trainx.append(train_x[end: end + left_out])\r\n",
        "  batch_trainy.append(train_y[end: end + left_out])"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTtmRF80Ji0y"
      },
      "source": [
        "# intializing the weights for each layer\r\n",
        "def intialize_weights(layer_dims):\r\n",
        "    np.random.seed(3)\r\n",
        "    parameters = {}\r\n",
        "    L = len(layer_dims)\r\n",
        "    \r\n",
        "    #initialise the value of weights based on the number of layers\r\n",
        "    for i in range(1,L-1):\r\n",
        "        #for the hidden layers we will use He initialisation because of relu activation\r\n",
        "        parameters['W'+str(i)] = np.random.randn(layer_dims[i-1],layer_dims[i]) * np.sqrt(2/layer_dims[i-1])\r\n",
        "        parameters['b'+str(i)] = np.zeros([1, layer_dims[i]])   \r\n",
        "            \r\n",
        "    #for the last layer we can use Xavier initialisation \r\n",
        "    parameters['W' + str(i+1)] = np.random.randn(layer_dims[i],layer_dims[i+1]) * np.sqrt(1/layer_dims[i])\r\n",
        "    parameters['b'+str(i+1)] = np.zeros([1, layer_dims[i+1]])    \r\n",
        "    \r\n",
        "    return parameters"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzhbJjLSJmH3"
      },
      "source": [
        "#forward propagation\r\n",
        "def forward_propagation(layer_dims,train_x,parameters, keep_probs,mode):\r\n",
        "    \r\n",
        "    caches = []\r\n",
        "    Aprev = train_x\r\n",
        "    L = len(layer_dims)\r\n",
        "    \r\n",
        "    #forward propagation for all the layers except last layer\r\n",
        "    for i in range(1,L-1): \r\n",
        "        W = parameters['W'+ str(i)]\r\n",
        "        b = parameters['b' + str(i)] \r\n",
        "        Z = np.dot(Aprev, W) + b  \r\n",
        "        Aprev = np.maximum(0,Z)       \r\n",
        "        cache = Aprev, W, b\r\n",
        "        caches.append(cache)     \r\n",
        "    \r\n",
        "    #forward propagation for the last layer\r\n",
        "    W = parameters['W'+ str(L-1)]\r\n",
        "    b = parameters['b' + str(L-1)]\r\n",
        "    Zlast = np.dot(Aprev, W) + b    \r\n",
        "    Alast = 1/(1 + np.exp(-Zlast))   \r\n",
        "    cache = Alast, W, b\r\n",
        "\r\n",
        "    caches.append(cache)\r\n",
        "    return caches"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcFUq25KJ1tB"
      },
      "source": [
        "#cost function calculation\r\n",
        "def cost_calculate(predict_y,train_y):\r\n",
        "    m = train_y.shape[0]\r\n",
        "    cost = -(np.dot(train_y.T, np.log(predict_y)) + np.dot((1-train_y).T, np.log(1-predict_y)))/m\r\n",
        "    return cost"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwTeuomaJ6gK"
      },
      "source": [
        "def backward_propagation(layer_dims, caches, parameters, train_x, train_y, learning_rate):\r\n",
        "    #backward propagation for the last layer\r\n",
        "    #Extract the last array from the caches, as this corresponds to the final output\r\n",
        "    L = len(caches)    \r\n",
        "    Acurr,Wcurr,bcurr = caches[L - 1]  \r\n",
        "    Aprev,Wprev,bprev = caches[L - 2]\r\n",
        "\r\n",
        "    m = train_y.shape[0]   \r\n",
        "    \r\n",
        "    dzprev = (Acurr - train_y)    \r\n",
        "    dwlast = np.dot(Aprev.T, dzprev)/m    \r\n",
        "    dblast = np.sum(dzprev, keepdims = True, axis = 0)/m        \r\n",
        "    parameters['W' + str(L)]= parameters['W' + str(L)] - (learning_rate * dwlast)    \r\n",
        "    parameters['b' + str(L)]= parameters['b' + str(L)] - (learning_rate * dblast)   \r\n",
        "            \r\n",
        "    for i in reversed(range(L-1)):\r\n",
        "        Anext,Wnext,bnext = caches[i+1]\r\n",
        "        Acurr,Wcurr,bcurr = caches[i]  \r\n",
        "        if i == 0:\r\n",
        "            Aprev = train_x\r\n",
        "        else:            \r\n",
        "            Aprev,Wprev,bprev = caches[i-2]\r\n",
        "                \r\n",
        "        dzcurr = np.where(Acurr > 0,1,Acurr)                     \r\n",
        "        dzprev = np.multiply(np.dot(dzprev,Wnext.T), dzcurr)\r\n",
        "        \r\n",
        "        dW = np.dot(Aprev.T,dzprev)/m\r\n",
        "        db = np.sum(dzprev, keepdims = True, axis = 0)/m  \r\n",
        "        parameters['W' + str(i+1)]= parameters['W' + str(i+1)] - (learning_rate * dW)\r\n",
        "        parameters['b' + str(i+1)]= parameters['b' + str(i+1)] - (learning_rate * db)     \r\n",
        "    return parameters"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CdrK6ajKUCt"
      },
      "source": [
        "def complete_model(layer_dims, train_x, train_y, learning_rate, iterations, keep_probs,mode):\r\n",
        "    \r\n",
        "    L = len(layer_dims)\r\n",
        "    # Intialize the weights\r\n",
        "    parameters = intialize_weights(layer_dims)\r\n",
        "    \r\n",
        "    for i in range(iterations):\r\n",
        "      for j in range(len(batch_trainx)):\r\n",
        "        #forward propagation\r\n",
        "        caches = forward_propagation(layer_dims,batch_trainx[j],parameters, keep_probs,mode)\r\n",
        "        \r\n",
        "        #calculate the cost \r\n",
        "        A,W,b = caches[-1]\r\n",
        "        cost = cost_calculate(A,batch_trainy[j])\r\n",
        "        if i%1000 == 0:\r\n",
        "            print('The cost after iteration {}: {}'.format(i, np.squeeze(cost)))\r\n",
        "        #backward propagation\r\n",
        "        parameters = backward_propagation(layer_dims, caches, parameters,batch_trainx[j], batch_trainy[j], learning_rate)\r\n",
        "\r\n",
        "    return parameters"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwC_7KL0KYe7",
        "outputId": "337aedfd-53e3-49f9-eb34-698e795288a2"
      },
      "source": [
        "layer_dims = [4,5,3,1]\r\n",
        "learning_rate = 0.15\r\n",
        "iterations = 5000\r\n",
        "keep_probs = 0.8\r\n",
        "mode = 'train'\r\n",
        "parameters = complete_model(layer_dims, train_x, train_y, learning_rate, iterations, keep_probs, mode)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cost after iteration 0: 0.5998951433889075\n",
            "The cost after iteration 0: 0.4940940578040702\n",
            "The cost after iteration 0: 0.435549694259776\n",
            "The cost after iteration 0: 0.5115873715437167\n",
            "The cost after iteration 0: 0.5166993127752532\n",
            "The cost after iteration 0: 0.5001224461195737\n",
            "The cost after iteration 0: 0.43298035075996216\n",
            "The cost after iteration 0: 0.22453076395646365\n",
            "The cost after iteration 0: 0.3652804271854403\n",
            "The cost after iteration 1000: 0.0010456320689345992\n",
            "The cost after iteration 1000: 0.00045916666005238753\n",
            "The cost after iteration 1000: 0.0007032498779519961\n",
            "The cost after iteration 1000: 0.0010466535059201589\n",
            "The cost after iteration 1000: 0.0010425825095893326\n",
            "The cost after iteration 1000: 0.0010419275588093916\n",
            "The cost after iteration 1000: 0.0008791508581443363\n",
            "The cost after iteration 1000: 0.0003805516174784836\n",
            "The cost after iteration 1000: 0.0007962785353275919\n",
            "The cost after iteration 2000: 0.0005193358979705577\n",
            "The cost after iteration 2000: 0.00022683479953508934\n",
            "The cost after iteration 2000: 0.0003486089092756077\n",
            "The cost after iteration 2000: 0.0005196938324116076\n",
            "The cost after iteration 2000: 0.0005174261925302713\n",
            "The cost after iteration 2000: 0.0005170130557444601\n",
            "The cost after iteration 2000: 0.0004362813426229842\n",
            "The cost after iteration 2000: 0.0001873627435888841\n",
            "The cost after iteration 2000: 0.0003969596864144686\n",
            "The cost after iteration 3000: 0.0003452155678504497\n",
            "The cost after iteration 3000: 0.00015007742629415924\n",
            "The cost after iteration 3000: 0.00023152875553391847\n",
            "The cost after iteration 3000: 0.000345276310527912\n",
            "The cost after iteration 3000: 0.00034396707042321726\n",
            "The cost after iteration 3000: 0.00034360742347080243\n",
            "The cost after iteration 3000: 0.0002898554718497599\n",
            "The cost after iteration 3000: 0.00012395382765693815\n",
            "The cost after iteration 3000: 0.0002644360000255692\n",
            "The cost after iteration 4000: 0.00025845431294024607\n",
            "The cost after iteration 4000: 0.00011190343341989014\n",
            "The cost after iteration 4000: 0.0001732409792841058\n",
            "The cost after iteration 4000: 0.0002583878651374385\n",
            "The cost after iteration 4000: 0.0002575256048760768\n",
            "The cost after iteration 4000: 0.00025723680724695617\n",
            "The cost after iteration 4000: 0.00021692489467931892\n",
            "The cost after iteration 4000: 9.247430964580847e-05\n",
            "The cost after iteration 4000: 0.00019827602906011632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEZw-e3RKa0g"
      },
      "source": [
        ""
      ],
      "execution_count": 98,
      "outputs": []
    }
  ]
}