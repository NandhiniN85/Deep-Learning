{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimization techniques.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NetvIWn4F5YZ"
      },
      "source": [
        "from sklearn.datasets import load_iris\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Activation, Dense\r\n",
        "from keras import optimizers\r\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnbqTKSIJ04r"
      },
      "source": [
        "iris = load_iris()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGBlwq_PJ284",
        "outputId": "d1d31259-5e8b-440d-a7a8-3dc427e3055f"
      },
      "source": [
        "iris.keys()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuDZvjhaJ4cj"
      },
      "source": [
        "data = pd.DataFrame(iris['data'], columns = iris['feature_names'] )\r\n",
        "target = pd.DataFrame(iris['target'],columns = ['target'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRiRJJWxKA0_"
      },
      "source": [
        "#combine the input predictors and target so that it can be split into training and testing\r\n",
        "data_target = data.join(target)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "ne9lk8_xKCP2",
        "outputId": "7fa6efc2-f698-4c68-9ed8-5bc85591cab8"
      },
      "source": [
        "data_target.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  ...  petal width (cm)  target\n",
              "0                5.1               3.5  ...               0.2       0\n",
              "1                4.9               3.0  ...               0.2       0\n",
              "2                4.7               3.2  ...               0.2       0\n",
              "3                4.6               3.1  ...               0.2       0\n",
              "4                5.0               3.6  ...               0.2       0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_Oo-KXAKDkP",
        "outputId": "df14412e-7117-48a6-914f-d003742741ef"
      },
      "source": [
        "data_target['target'].value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    50\n",
              "1    50\n",
              "0    50\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_437VFFmKGZy"
      },
      "source": [
        "#for this implementaion, let's make the problem a binary. So considering only '0' and '1' as the target\r\n",
        "X = np.array(data_target[(data_target['target'] == 0) | (data_target['target'] == 1)].drop('target', axis=1))\r\n",
        "y = np.array(data_target[(data_target['target'] == 0) | (data_target['target'] == 1)]['target']).reshape(100,1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBJM3vydKLGI",
        "outputId": "fc468927-7bd6-401d-f913-0fac8f6faa7e"
      },
      "source": [
        "train_x,test_x,train_y,test_y = train_test_split(X,y,test_size = 0.2)\r\n",
        "\r\n",
        "print('Shape of train_x:', train_x.shape)\r\n",
        "print('Shape of train_y:', train_y.shape)\r\n",
        "print('Shape of test_x:', test_x.shape)\r\n",
        "print('Shape of test_y:', test_y.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of train_x: (80, 4)\n",
            "Shape of train_y: (80, 1)\n",
            "Shape of test_x: (20, 4)\n",
            "Shape of test_y: (20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs-ZdRE5T4H1"
      },
      "source": [
        "Splitting the data into 9 mini batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30cZ3UUyT0zL"
      },
      "source": [
        "batches = 9\r\n",
        "\r\n",
        "#get the total number of batches\r\n",
        "batch_count = train_x.shape[0] // batches\r\n",
        "\r\n",
        "batch_trainx = []\r\n",
        "batch_trainy = []\r\n",
        "\r\n",
        "for i in range(0, batch_count):\r\n",
        "  begin = i * batches\r\n",
        "  end = (i + 1) * batches\r\n",
        "\r\n",
        "  batch_trainx.append(train_x[begin:end])\r\n",
        "  batch_trainy.append(train_y[begin:end])\r\n",
        "\r\n",
        "#when the total count is not exactly divisible by batches\r\n",
        "left_out = train_x.shape[0] % batches\r\n",
        "\r\n",
        "if left_out != 0:\r\n",
        "  batch_trainx.append(train_x[end: end + left_out])\r\n",
        "  batch_trainy.append(train_y[end: end + left_out])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23ZNy6PdKMj3"
      },
      "source": [
        "# intializing the weights for each layer\r\n",
        "def intialize_weights(layer_dims):\r\n",
        "    np.random.seed(3)\r\n",
        "    parameters = {}\r\n",
        "    L = len(layer_dims)\r\n",
        "    \r\n",
        "    #initialise the value of weights based on the number of layers\r\n",
        "    for i in range(1,L-1):\r\n",
        "        #for the hidden layers we will use He initialisation because of relu activation\r\n",
        "        parameters['W'+str(i)] = np.random.randn(layer_dims[i-1],layer_dims[i]) * np.sqrt(2/layer_dims[i-1])\r\n",
        "        parameters['b'+str(i)] = np.zeros([1, layer_dims[i]])   \r\n",
        "            \r\n",
        "    #for the last layer we can use Xavier initialisation \r\n",
        "    parameters['W' + str(i+1)] = np.random.randn(layer_dims[i],layer_dims[i+1]) * np.sqrt(1/layer_dims[i])\r\n",
        "    parameters['b'+str(i+1)] = np.zeros([1, layer_dims[i+1]])    \r\n",
        "    \r\n",
        "    return parameters"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrMdOuUFKPru"
      },
      "source": [
        "#forward propagation\r\n",
        "def forward_propagation(layer_dims,train_x,parameters, keep_probs,mode):\r\n",
        "    \r\n",
        "    caches = []\r\n",
        "    Aprev = train_x\r\n",
        "    L = len(layer_dims)\r\n",
        "    \r\n",
        "    #forward propagation for all the layers except last layer\r\n",
        "    for i in range(1,L-1): \r\n",
        "        W = parameters['W'+ str(i)]\r\n",
        "        b = parameters['b' + str(i)] \r\n",
        "        Z = np.dot(Aprev, W) + b  \r\n",
        "        Aprev = np.maximum(0,Z)   \r\n",
        "        if mode == 'train':\r\n",
        "          drop = np.random.rand(Aprev.shape[0],Aprev.shape[1])\r\n",
        "          drop = (drop < keep_probs).astype(int)\r\n",
        "          Aprev = np.multiply(Aprev, drop)\r\n",
        "          Aprev = np.divide(Aprev,keep_probs)     \r\n",
        "          cache = Aprev, W, b, drop\r\n",
        "          caches.append(cache)     \r\n",
        "        else:\r\n",
        "          cache = Aprev, W, b\r\n",
        "          caches.append(cache)\r\n",
        "    \r\n",
        "    #forward propagation for the last layer\r\n",
        "    W = parameters['W'+ str(L-1)]\r\n",
        "    b = parameters['b' + str(L-1)]\r\n",
        "    Zlast = np.dot(Aprev, W) + b    \r\n",
        "    Alast = 1/(1 + np.exp(-Zlast))   \r\n",
        "    if mode == 'train':      \r\n",
        "      drop = 0  #dummy value since in the last layer we do not have any dropout\r\n",
        "      cache = Alast, W, b, drop\r\n",
        "    else:\r\n",
        "      cache = Alast, W, b\r\n",
        "\r\n",
        "    caches.append(cache)\r\n",
        "    return caches"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XJ_H0gdKSiC"
      },
      "source": [
        "#cost function calculation\r\n",
        "def cost_calculate(predict_y,train_y):\r\n",
        "    m = train_y.shape[0]\r\n",
        "    cost = -(np.dot(train_y.T, np.log(predict_y)) + np.dot((1-train_y).T, np.log(1-predict_y)))/m\r\n",
        "    return cost"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs_4agdOKUDp"
      },
      "source": [
        "def backward_propagation(layer_dims, caches, parameters, train_y, learning_rate):\r\n",
        "    #backward propagation for the last layer\r\n",
        "    #Extract the last array from the caches, as this corresponds to the final output\r\n",
        "    L = len(layer_dims)    \r\n",
        "    Acurr,Wcurr,bcurr,dcurr = caches[L - 2]  \r\n",
        "    Aprev,Wprev,bprev,dprev = caches[L - 3]\r\n",
        "    \r\n",
        "    m = train_y.shape[0]    \r\n",
        "    \r\n",
        "    dzprev = (Acurr - train_y)    \r\n",
        "    dwlast = np.dot(Aprev.T, dzprev)/m    \r\n",
        "    dblast = np.sum(dzprev, keepdims = True, axis = 0)/m        \r\n",
        "    parameters['W' + str(L-1)]= parameters['W' + str(L-1)] - (learning_rate * dwlast)    \r\n",
        "    parameters['b' + str(L-1)]= parameters['b' + str(L-1)] - (learning_rate * dblast)    \r\n",
        "        \r\n",
        "    for i in reversed(range(L-2)):\r\n",
        "        Anext,Wnext,bnext,dnext = caches[i+1]\r\n",
        "        Acurr,Wcurr,bcurr,dcurr = caches[i]  \r\n",
        "        if i == 0:\r\n",
        "            Aprev = train_x\r\n",
        "        else:            \r\n",
        "            Aprev,Wprev,bprev,dprev = caches[i-1]\r\n",
        "                \r\n",
        "        dzcurr = np.where(Acurr > 0,1,Acurr)             \r\n",
        "        da = np.dot(dzprev,Wnext.T)\r\n",
        "        da = da * dcurr\r\n",
        "        da = da / keep_probs\r\n",
        "        dzprev = np.multiply(da, dzcurr)\r\n",
        "        \r\n",
        "        dW = np.dot(Aprev.T,dzprev)/m\r\n",
        "        db = np.sum(dzprev, keepdims = True, axis = 0)/m  \r\n",
        "        parameters['W' + str(i+1)]= parameters['W' + str(i+1)] - (learning_rate * dW)\r\n",
        "        parameters['b' + str(i+1)]= parameters['b' + str(i+1)] - (learning_rate * db)     \r\n",
        "        return parameters"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTnYh6OoKWCR"
      },
      "source": [
        "def complete_model(layer_dims, train_x, train_y, learning_rate, iterations, keep_probs,mode):\r\n",
        "    \r\n",
        "    L = len(layer_dims)\r\n",
        "    # Intialize the weights\r\n",
        "    parameters = intialize_weights(layer_dims)\r\n",
        "    \r\n",
        "    for i in range(iterations):\r\n",
        "      for j in range(len(batch_trainx)):\r\n",
        "        #forward propagation\r\n",
        "        caches = forward_propagation(layer_dims,batch_trainx[j],parameters, keep_probs,mode)\r\n",
        "        \r\n",
        "        #calculate the cost \r\n",
        "        A,W,b,d = caches[-1]\r\n",
        "        cost = cost_calculate(A,batch_trainy[j])\r\n",
        "        if i%1000 == 0:\r\n",
        "            print('The cost after iteration {}: {}'.format(i, np.squeeze(cost)))\r\n",
        "                  \r\n",
        "        #backward propagation\r\n",
        "        parameters = backward_propagation(layer_dims, caches, parameters, batch_trainy[j], learning_rate)\r\n",
        "    return parameters"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgJPiX1ZKYdC",
        "outputId": "4c705947-b033-4f10-f742-828d45db3d39"
      },
      "source": [
        "layer_dims = [4,5,3,1]\r\n",
        "learning_rate = 0.15\r\n",
        "iterations = 14900\r\n",
        "keep_probs = 0.8\r\n",
        "mode = 'train'\r\n",
        "parameters = complete_model(layer_dims, train_x, train_y, learning_rate, iterations, keep_probs, mode)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cost after iteration 0: 0.6138894562663282\n",
            "The cost after iteration 0: 0.5473925554083254\n",
            "The cost after iteration 0: 0.7017106912881773\n",
            "The cost after iteration 0: 0.7104632451588682\n",
            "The cost after iteration 0: 0.43945906478925373\n",
            "The cost after iteration 0: 0.5600286738519666\n",
            "The cost after iteration 0: 0.3104161910189528\n",
            "The cost after iteration 0: 0.591508932094064\n",
            "The cost after iteration 0: 0.48312027756113074\n",
            "The cost after iteration 1000: 0.17265964636077646\n",
            "The cost after iteration 1000: 0.58106252378826\n",
            "The cost after iteration 1000: 0.17230870130325293\n",
            "The cost after iteration 1000: 0.20277520941692315\n",
            "The cost after iteration 1000: 0.13262145635179406\n",
            "The cost after iteration 1000: 0.1960976275107916\n",
            "The cost after iteration 1000: 0.68183886768904\n",
            "The cost after iteration 1000: 0.4010088547027096\n",
            "The cost after iteration 1000: 0.6216345146642516\n",
            "The cost after iteration 2000: 0.6042708529614802\n",
            "The cost after iteration 2000: 0.4276269648145121\n",
            "The cost after iteration 2000: 0.1900417654964269\n",
            "The cost after iteration 2000: 0.22341770680304032\n",
            "The cost after iteration 2000: 0.28730381152077755\n",
            "The cost after iteration 2000: 0.21990809655358035\n",
            "The cost after iteration 2000: 0.5045365324370613\n",
            "The cost after iteration 2000: 0.5356732502385203\n",
            "The cost after iteration 2000: 0.28331643068453105\n",
            "The cost after iteration 3000: 0.3259641556837411\n",
            "The cost after iteration 3000: 0.2880886021503824\n",
            "The cost after iteration 3000: 0.32451483858501784\n",
            "The cost after iteration 3000: 0.2182373414793327\n",
            "The cost after iteration 3000: 0.718119418299194\n",
            "The cost after iteration 3000: 0.3602475556442363\n",
            "The cost after iteration 3000: 0.35829874380516435\n",
            "The cost after iteration 3000: 0.8175613316444899\n",
            "The cost after iteration 3000: 0.13157043847805716\n",
            "The cost after iteration 4000: 0.4658032812236274\n",
            "The cost after iteration 4000: 0.5591824240196035\n",
            "The cost after iteration 4000: 0.3338722755385455\n",
            "The cost after iteration 4000: 0.23933268824260776\n",
            "The cost after iteration 4000: 0.15555660613479513\n",
            "The cost after iteration 4000: 0.5036253291835149\n",
            "The cost after iteration 4000: 0.4886674218782119\n",
            "The cost after iteration 4000: 0.5231548391776177\n",
            "The cost after iteration 4000: 0.5854437162682394\n",
            "The cost after iteration 5000: 0.1887989439644982\n",
            "The cost after iteration 5000: 0.568508432062513\n",
            "The cost after iteration 5000: 0.46532254370062176\n",
            "The cost after iteration 5000: 0.2251864618901508\n",
            "The cost after iteration 5000: 0.2876244693758508\n",
            "The cost after iteration 5000: 0.36016011959450905\n",
            "The cost after iteration 5000: 0.21482318596182617\n",
            "The cost after iteration 5000: 0.3934585133568599\n",
            "The cost after iteration 5000: 0.12274423833567234\n",
            "The cost after iteration 6000: 0.4658889457791566\n",
            "The cost after iteration 6000: 0.28517118363396166\n",
            "The cost after iteration 6000: 0.3196002170969023\n",
            "The cost after iteration 6000: 0.35327513355375767\n",
            "The cost after iteration 6000: 0.28405720555225417\n",
            "The cost after iteration 6000: 0.203044461867619\n",
            "The cost after iteration 6000: 0.36788940482977184\n",
            "The cost after iteration 6000: 0.10046606914043721\n",
            "The cost after iteration 6000: 0.11181488114102585\n",
            "The cost after iteration 7000: 0.465769723006056\n",
            "The cost after iteration 7000: 0.2925213685086856\n",
            "The cost after iteration 7000: 0.3315385882384224\n",
            "The cost after iteration 7000: 0.5045781198482855\n",
            "The cost after iteration 7000: 0.4266404447712945\n",
            "The cost after iteration 7000: 0.2338651199665286\n",
            "The cost after iteration 7000: 0.3513313028761586\n",
            "The cost after iteration 7000: 0.25193618337550966\n",
            "The cost after iteration 7000: 0.43725090995346344\n",
            "The cost after iteration 8000: 0.3261345099813136\n",
            "The cost after iteration 8000: 0.28824826883812144\n",
            "The cost after iteration 8000: 0.32466313547803\n",
            "The cost after iteration 8000: 0.3602225453069774\n",
            "The cost after iteration 8000: 0.5724471783304667\n",
            "The cost after iteration 8000: 0.36769627417773015\n",
            "The cost after iteration 8000: 0.78544783688968\n",
            "The cost after iteration 8000: 0.39008323840499454\n",
            "The cost after iteration 8000: 0.28351843506301294\n",
            "The cost after iteration 9000: 0.32094931201903143\n",
            "The cost after iteration 9000: 0.13971680673862696\n",
            "The cost after iteration 9000: 0.31900613256010085\n",
            "The cost after iteration 9000: 0.35244767076846667\n",
            "The cost after iteration 9000: 0.283764793538947\n",
            "The cost after iteration 9000: 0.20237012913960348\n",
            "The cost after iteration 9000: 0.5203975318406614\n",
            "The cost after iteration 9000: 0.8469615261221927\n",
            "The cost after iteration 9000: 0.7719779701939972\n",
            "The cost after iteration 10000: 0.4654191706944327\n",
            "The cost after iteration 10000: 0.2904385428620097\n",
            "The cost after iteration 10000: 0.46537693729254376\n",
            "The cost after iteration 10000: 0.22929628432022695\n",
            "The cost after iteration 10000: 0.5674862563854096\n",
            "The cost after iteration 10000: 0.5031534876512854\n",
            "The cost after iteration 10000: 0.3523737139060171\n",
            "The cost after iteration 10000: 0.6639611486069084\n",
            "The cost after iteration 10000: 0.5873584813956035\n",
            "The cost after iteration 11000: 0.20262799230930026\n",
            "The cost after iteration 11000: 0.15867917898809425\n",
            "The cost after iteration 11000: 0.3302741729489505\n",
            "The cost after iteration 11000: 0.5040127415651954\n",
            "The cost after iteration 11000: 0.2902901198040309\n",
            "The cost after iteration 11000: 0.22980791136564604\n",
            "The cost after iteration 11000: 0.49328979310313925\n",
            "The cost after iteration 11000: 0.25171471795787526\n",
            "The cost after iteration 11000: 0.4381846719888792\n",
            "The cost after iteration 12000: 0.46535448437174665\n",
            "The cost after iteration 12000: 0.42732497366055705\n",
            "The cost after iteration 12000: 0.46536834250221926\n",
            "The cost after iteration 12000: 0.2288960518371049\n",
            "The cost after iteration 12000: 0.28849324042423863\n",
            "The cost after iteration 12000: 0.6425019577705497\n",
            "The cost after iteration 12000: 0.7723360685818607\n",
            "The cost after iteration 12000: 0.6590246050062306\n",
            "The cost after iteration 12000: 0.4342131828574948\n",
            "The cost after iteration 13000: 0.3144541665603025\n",
            "The cost after iteration 13000: 0.28195410376042257\n",
            "The cost after iteration 13000: 0.6221545574531413\n",
            "The cost after iteration 13000: 0.49987132882928254\n",
            "The cost after iteration 13000: 0.2822986320973561\n",
            "The cost after iteration 13000: 0.4998615893598248\n",
            "The cost after iteration 13000: 0.21771200158600154\n",
            "The cost after iteration 13000: 0.4028320647496888\n",
            "The cost after iteration 13000: 0.28129547508097064\n",
            "The cost after iteration 14000: 0.46532084170725657\n",
            "The cost after iteration 14000: 0.42796229683988013\n",
            "The cost after iteration 14000: 0.46532206108864166\n",
            "The cost after iteration 14000: 0.6417181552754625\n",
            "The cost after iteration 14000: 0.5659262781373279\n",
            "The cost after iteration 14000: 0.3670337535242044\n",
            "The cost after iteration 14000: 0.4906065805018607\n",
            "The cost after iteration 14000: 0.524680739885671\n",
            "The cost after iteration 14000: 0.4355196488923387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt_L1YkqKalp"
      },
      "source": [
        "mode = 'test'\r\n",
        "predict = forward_propagation(layer_dims,test_x,parameters,keep_probs,mode)[-1][0]\r\n",
        "test_cost = cost_calculate(predict,test_y)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omv-4aHpKcdh",
        "outputId": "a50cd750-65d7-4ec3-924e-8bef8cd126e9"
      },
      "source": [
        "test_cost"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.3930724]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0_lp6-3KdyM"
      },
      "source": [
        "model = Sequential()\r\n",
        "model.add(Dense(50, input_shape = (4, ),kernel_initializer='he_normal'))\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(50,kernel_initializer='he_normal'))\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "model.add(Dense(1,kernel_initializer='glorot_normal'))\r\n",
        "model.add(Activation('sigmoid'))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkGdwpdeKfUe"
      },
      "source": [
        "sgd = optimizers.SGD(lr = 0.01)\r\n",
        "model.compile(loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y01sZsRKqaZ",
        "outputId": "73e3b989-e693-4c70-dffd-62c52b4d57f9"
      },
      "source": [
        "model.fit(train_x, train_y, validation_data = (test_x, test_y), epochs = 30, batch_size=9)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "9/9 [==============================] - 3s 53ms/step - loss: 2.3907 - accuracy: 0.4328 - val_loss: 0.4773 - val_accuracy: 0.9500\n",
            "Epoch 2/30\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.3067 - accuracy: 0.5970 - val_loss: 0.2692 - val_accuracy: 1.0000\n",
            "Epoch 3/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 1.8877 - accuracy: 0.6003 - val_loss: 0.1190 - val_accuracy: 1.0000\n",
            "Epoch 4/30\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 1.3689 - accuracy: 0.5675 - val_loss: 0.0659 - val_accuracy: 1.0000\n",
            "Epoch 5/30\n",
            "9/9 [==============================] - 0s 20ms/step - loss: 1.1775 - accuracy: 0.5859 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
            "Epoch 6/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.5345 - accuracy: 0.7536 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
            "Epoch 7/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3825 - accuracy: 0.8501 - val_loss: 0.0161 - val_accuracy: 1.0000\n",
            "Epoch 8/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.2679 - accuracy: 0.8839 - val_loss: 0.0158 - val_accuracy: 1.0000\n",
            "Epoch 9/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3314 - accuracy: 0.8854 - val_loss: 0.0114 - val_accuracy: 1.0000\n",
            "Epoch 10/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3619 - accuracy: 0.8796 - val_loss: 0.0146 - val_accuracy: 1.0000\n",
            "Epoch 11/30\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.1385 - accuracy: 0.9308 - val_loss: 0.0113 - val_accuracy: 1.0000\n",
            "Epoch 12/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.2239 - accuracy: 0.9180 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
            "Epoch 13/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1637 - accuracy: 0.9359 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 14/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1853 - accuracy: 0.9460 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 15/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1189 - accuracy: 0.9759 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
            "Epoch 16/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0442 - accuracy: 0.9784 - val_loss: 0.0037 - val_accuracy: 1.0000\n",
            "Epoch 17/30\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1145 - accuracy: 0.9198 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
            "Epoch 18/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1168 - accuracy: 0.9526 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
            "Epoch 19/30\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
            "Epoch 20/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0324 - accuracy: 0.9961 - val_loss: 8.2611e-04 - val_accuracy: 1.0000\n",
            "Epoch 21/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0851 - accuracy: 0.9784 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 22/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0594 - accuracy: 0.9578 - val_loss: 4.3368e-04 - val_accuracy: 1.0000\n",
            "Epoch 23/30\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0408 - accuracy: 0.9975 - val_loss: 3.2598e-04 - val_accuracy: 1.0000\n",
            "Epoch 24/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1236 - accuracy: 0.9488 - val_loss: 3.1732e-04 - val_accuracy: 1.0000\n",
            "Epoch 25/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 3.4707e-04 - val_accuracy: 1.0000\n",
            "Epoch 26/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.2153 - accuracy: 0.9130 - val_loss: 2.9158e-04 - val_accuracy: 1.0000\n",
            "Epoch 27/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0335 - accuracy: 0.9784 - val_loss: 2.6038e-04 - val_accuracy: 1.0000\n",
            "Epoch 28/30\n",
            "9/9 [==============================] - 0s 8ms/step - loss: 0.1283 - accuracy: 0.9784 - val_loss: 8.4995e-04 - val_accuracy: 1.0000\n",
            "Epoch 29/30\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0709 - accuracy: 0.9634 - val_loss: 2.4108e-04 - val_accuracy: 1.0000\n",
            "Epoch 30/30\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 1.9184e-04 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f71a01387b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxDreEFLKsPn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}